{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e42d75",
   "metadata": {},
   "outputs": [],
   "source": [
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# AIM BOT Detection in CS:GO\\n\",\n",
    "    \"## COPS Summer of Code 2025 - Intelligence Guild\\n\",\n",
    "    \"### Computer Vision Week 2 Assignment\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.optim as optim\\n\",\n",
    "    \"from torch.utils.data import Dataset, DataLoader, random_split\\n\",\n",
    "    \"import torchvision.transforms as transforms\\n\",\n",
    "    \"from torchvision.models.video import r3d_18\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import time\\n\",\n",
    "    \"import yaml\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configuration\\n\",\n",
    "    \"config = {\\n\",\n",
    "    \"    'data_path': 'data/',\\n\",\n",
    "    \"    'batch_size': 8,\\n\",\n",
    "    \"    'epochs': 50,\\n\",\n",
    "    \"    'lr': 0.001,\\n\",\n",
    "    \"    'frames_per_clip': 30,\\n\",\n",
    "    \"    'img_size': (128, 72),\\n\",\n",
    "    \"    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\\n\",\n",
    "    \"    'seed': 42\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set seed for reproducibility\\n\",\n",
    "    \"torch.manual_seed(config['seed'])\\n\",\n",
    "    \"np.random.seed(config['seed'])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class CSGODataset(Dataset):\\n\",\n",
    "    \"    def __init__(self, data_dir, transform=None):\\n\",\n",
    "    \"        self.data_dir = data_dir\\n\",\n",
    "    \"        self.transform = transform\\n\",\n",
    "    \"        self.classes = {'clean': 0, 'aimbot': 1}\\n\",\n",
    "    \"        self.samples = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for label_name, label_val in self.classes.items():\\n\",\n",
    "    \"            class_dir = os.path.join(data_dir, label_name)\\n\",\n",
    "    \"            for video_file in os.listdir(class_dir):\\n\",\n",
    "    \"                if video_file.endswith('.mp4'):\\n\",\n",
    "    \"                    self.samples.append((os.path.join(class_dir, video_file), label_val))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __len__(self):\\n\",\n",
    "    \"        return len(self.samples)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __getitem__(self, idx):\\n\",\n",
    "    \"        video_path, label = self.samples[idx]\\n\",\n",
    "    \"        frames = []\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        cap = cv2.VideoCapture(video_path)\\n\",\n",
    "    \"        while cap.isOpened():\\n\",\n",
    "    \"            ret, frame = cap.read()\\n\",\n",
    "    \"            if not ret:\\n\",\n",
    "    \"                break\\n\",\n",
    "    \"            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"            frame = cv2.resize(frame, config['img_size'])\\n\",\n",
    "    \"            frames.append(frame)\\n\",\n",
    "    \"        cap.release()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Sample fixed-length clip\\n\",\n",
    "    \"        if len(frames) > config['frames_per_clip']:\\n\",\n",
    "    \"            start_idx = np.random.randint(0, len(frames) - config['frames_per_clip'])\\n\",\n",
    "    \"            frames = frames[start_idx:start_idx+config['frames_per_clip']]\\n\",\n",
    "    \"        elif len(frames) < config['frames_per_clip']:\\n\",\n",
    "    \"            # Pad with last frame\\n\",\n",
    "    \"            frames += [frames[-1]] * (config['frames_per_clip'] - len(frames))\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Convert to tensor (T, H, W, C) -> (C, T, H, W)\\n\",\n",
    "    \"        clip = torch.tensor(np.array(frames)).permute(3, 0, 1, 2).float() / 255.0\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if self.transform:\\n\",\n",
    "    \"            clip = self.transform(clip)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        return clip, label\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class AIMBotDetector(nn.Module):\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        # Use pre-trained 3D ResNet\\n\",\n",
    "    \"        self.backbone = r3d_18(pretrained=True)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Replace final layer\\n\",\n",
    "    \"        num_features = self.backbone.fc.in_features\\n\",\n",
    "    \"        self.backbone.fc = nn.Sequential(\\n\",\n",
    "    \"            nn.Linear(num_features, 512),\\n\",\n",
    "    \"            nn.ReLU(),\\n\",\n",
    "    \"            nn.Dropout(0.5),\\n\",\n",
    "    \"            nn.Linear(512, 2)\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Attention mechanism\\n\",\n",
    "    \"        self.attention = nn.Sequential(\\n\",\n",
    "    \"            nn.Conv3d(512, 1, kernel_size=1),\\n\",\n",
    "    \"            nn.Sigmoid()\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        # Extract features\\n\",\n",
    "    \"        features = self.backbone.stem(x)\\n\",\n",
    "    \"        features = self.backbone.layer1(features)\\n\",\n",
    "    \"        features = self.backbone.layer2(features)\\n\",\n",
    "    \"        features = self.backbone.layer3(features)\\n\",\n",
    "    \"        features = self.backbone.layer4(features)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Apply attention\\n\",\n",
    "    \"        attn_weights = self.attention(features)\\n\",\n",
    "    \"        attn_features = features * attn_weights\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Pooling\\n\",\n",
    "    \"        pooled = nn.functional.adaptive_avg_pool3d(attn_features, (1, 1, 1))\\n\",\n",
    "    \"        pooled = torch.flatten(pooled, 1)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Classification\\n\",\n",
    "    \"        return self.backbone.fc(pooled), attn_weights\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data augmentation\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.RandomHorizontalFlip(p=0.5),\\n\",\n",
    "    \"    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\\n\",\n",
    "    \"    transforms.RandomRotation(10)\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create dataset\\n\",\n",
    "    \"full_dataset = CSGODataset(config['data_path'], transform=transform)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split dataset\\n\",\n",
    "    \"train_size = int(0.8 * len(full_dataset))\\n\",\n",
    "    \"val_size = len(full_dataset) - train_size\\n\",\n",
    "    \"train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create data loaders\\n\",\n",
    "    \"train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\\n\",\n",
    "    \"val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize model\\n\",\n",
    "    \"model = AIMBotDetector().to(config['device'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Loss and optimizer\\n\",\n",
    "    \"criterion = nn.CrossEntropyLoss()\\n\",\n",
    "    \"optimizer = optim.Adam(model.parameters(), lr=config['lr'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Training loop\\n\",\n",
    "    \"train_losses, val_losses = [], []\\n\",\n",
    "    \"train_accs, val_accs = [], []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for epoch in range(config['epochs']):\\n\",\n",
    "    \"    # Training\\n\",\n",
    "    \"    model.train()\\n\",\n",
    "    \"    running_loss = 0.0\\n\",\n",
    "    \"    correct = 0\\n\",\n",
    "    \"    total = 0\\n\",\n",
    "    \"    start_time = time.time()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for inputs, labels in train_loader:\\n\",\n",
    "    \"        inputs, labels = inputs.to(config['device']), labels.to(config['device'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        optimizer.zero_grad()\\n\",\n",
    "    \"        outputs, _ = model(inputs)\\n\",\n",
    "    \"        loss = criterion(outputs, labels)\\n\",\n",
    "    \"        loss.backward()\\n\",\n",
    "    \"        optimizer.step()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        running_loss += loss.item()\\n\",\n",
    "    \"        _, predicted = outputs.max(1)\\n\",\n",
    "    \"        total += labels.size(0)\\n\",\n",
    "    \"        correct += predicted.eq(labels).sum().item()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    train_loss = running_loss / len(train_loader)\\n\",\n",
    "    \"    train_acc = 100 * correct / total\\n\",\n",
    "    \"    train_losses.append(train_loss)\\n\",\n",
    "    \"    train_accs.append(train_acc)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Validation\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    val_loss = 0.0\\n\",\n",
    "    \"    correct = 0\\n\",\n",
    "    \"    total = 0\\n\",\n",
    "    \"    all_preds = []\\n\",\n",
    "    \"    all_labels = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        for inputs, labels in val_loader:\\n\",\n",
    "    \"            inputs, labels = inputs.to(config['device']), labels.to(config['device'])\\n\",\n",
    "    \"            outputs, _ = model(inputs)\\n\",\n",
    "    \"            loss = criterion(outputs, labels)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            val_loss += loss.item()\\n\",\n",
    "    \"            _, predicted = outputs.max(1)\\n\",\n",
    "    \"            total += labels.size(0)\\n\",\n",
    "    \"            correct += predicted.eq(labels).sum().item()\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            all_preds.extend(predicted.cpu().numpy())\\n\",\n",
    "    \"            all_labels.extend(labels.cpu().numpy())\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    val_loss = val_loss / len(val_loader)\\n\",\n",
    "    \"    val_acc = 100 * correct / total\\n\",\n",
    "    \"    val_losses.append(val_loss)\\n\",\n",
    "    \"    val_accs.append(val_acc)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    epoch_time = time.time() - start_time\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"Epoch {epoch+1}/{config['epochs']} | \\\"\\n\",\n",
    "    \"          f\\\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \\\"\\n\",\n",
    "    \"          f\\\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% | \\\"\\n\",\n",
    "    \"          f\\\"Time: {epoch_time:.2f}s\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save model\\n\",\n",
    "    \"torch.save(model.state_dict(), 'aimbot_detector.pth')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot results\\n\",\n",
    "    \"plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"plt.plot(train_losses, label='Train Loss')\\n\",\n",
    "    \"plt.plot(val_losses, label='Val Loss')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.title('Loss Curve')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"plt.plot(train_accs, label='Train Accuracy')\\n\",\n",
    "    \"plt.plot(val_accs, label='Val Accuracy')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.title('Accuracy Curve')\\n\",\n",
    "    \"plt.savefig('training_results.png')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Classification report\\n\",\n",
    "    \"print(\\\"\\\\nClassification Report:\\\")\\n\",\n",
    "    \"print(classification_report(all_labels, all_preds, target_names=['Clean', 'Aimbot']))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion matrix\\n\",\n",
    "    \"cm = confusion_matrix(all_labels, all_preds)\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\\n\",\n",
    "    \"plt.title('Confusion Matrix')\\n\",\n",
    "    \"plt.colorbar()\\n\",\n",
    "    \"tick_marks = np.arange(2)\\n\",\n",
    "    \"plt.xticks(tick_marks, ['Clean', 'Aimbot'])\\n\",\n",
    "    \"plt.yticks(tick_marks, ['Clean', 'Aimbot'])\\n\",\n",
    "    \"plt.ylabel('True Label')\\n\",\n",
    "    \"plt.xlabel('Predicted Label')\\n\",\n",
    "    \"plt.savefig('confusion_matrix.png')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualization function\\n\",\n",
    "    \"def visualize_attention(video_path):\\n\",\n",
    "    \"    cap = cv2.VideoCapture(video_path)\\n\",\n",
    "    \"    frames = []\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while cap.isOpened():\\n\",\n",
    "    \"        ret, frame = cap.read()\\n\",\n",
    "    \"        if not ret:\\n\",\n",
    "    \"            break\\n\",\n",
    "    \"        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\\n\",\n",
    "    \"        frame = cv2.resize(frame, config['img_size'])\\n\",\n",
    "    \"        frames.append(frame)\\n\",\n",
    "    \"    cap.release()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Process clip\\n\",\n",
    "    \"    clip = torch.tensor(np.array(frames[:config['frames_per_clip']])).permute(3, 0, 1, 2).float() / 255.0\\n\",\n",
    "    \"    clip = clip.unsqueeze(0).to(config['device'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get predictions and attention\\n\",\n",
    "    \"    model.eval()\\n\",\n",
    "    \"    with torch.no_grad():\\n\",\n",
    "    \"        pred, attn_weights = model(clip)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Process attention\\n\",\n",
    "    \"    attn_weights = attn_weights.squeeze().cpu().numpy()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create overlay\\n\",\n",
    "    \"    for i in range(len(frames)):\\n\",\n",
    "    \"        frame = frames[i]\\n\",\n",
    "    \"        heatmap = cv2.resize(attn_weights[i], (frame.shape[1], frame.shape[0]))\\n\",\n",
    "    \"        heatmap = (heatmap * 255).astype(np.uint8)\\n\",\n",
    "    \"        heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\\n\",\n",
    "    \"        overlay = cv2.addWeighted(frame, 0.7, heatmap, 0.3, 0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"        plt.subplot(1, 2, 1)\\n\",\n",
    "    \"        plt.imshow(frame)\\n\",\n",
    "    \"        plt.title('Original Frame')\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.subplot(1, 2, 2)\\n\",\n",
    "    \"        plt.imshow(overlay)\\n\",\n",
    "    \"        plt.title('Attention Map')\\n\",\n",
    "    \"        plt.axis('off')\\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Example usage\\n\",\n",
    "    \"# visualize_attention('data/aimbot/sample.mp4')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Key Features of the Implementation\\n\",\n",
    "    \"1. **3D CNN Architecture**: Uses PyTorch's pre-trained R3D-18 model for spatiotemporal feature extraction\\n\",\n",
    "    \"2. **Attention Mechanism**: Visualizes regions of interest in gameplay frames\\n\",\n",
    "    \"3. **Data Augmentation**: Horizontal flips, color jitter, and rotation for robustness\\n\",\n",
    "    \"4. **Visualization Tools**: Attention mapping and performance metrics\\n\",\n",
    "    \"5. **Efficient Processing**: Frame sampling and GPU acceleration\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Usage Instructions\\n\",\n",
    "    \"1. Create dataset folder structure:\\n\",\n",
    "    \"   ```\\n\",\n",
    "    \"   data/\\n\",\n",
    "    \"   ├── aimbot/\\n\",\n",
    "    \"   │   ├── video1.mp4\\n\",\n",
    "    \"   │   └── ...\\n\",\n",
    "    \"   └── clean/\\n\",\n",
    "    \"       ├── video1.mp4\\n\",\n",
    "    \"       └── ...\\n\",\n",
    "    \"   ```\\n\",\n",
    "    \"2. Run all notebook cells\\n\",\n",
    "    \"3. Visualize results with `visualize_attention()` function\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
