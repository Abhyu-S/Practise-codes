{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding an artificial Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a Neuron work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2.0\n",
    "\n",
    "#Each neuron has only 1 bias\n",
    "\n",
    "ooutput = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "ooutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2.0\n",
    "\n",
    "ooutput = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "ooutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Inputs into 3 Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3,]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the real thing using Matrices and Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer_outputs = []\n",
    "for neuron_weights, neuron_bias in zip(weights, biases): # zip makes it a bundle\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = np.dot(np.array(inputs), np.array(weights).T) + biases #order matters here, thus weights some  before inputs\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing things in batches / Parallel computing (using GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5031 , -1.04185, -2.03875],\n",
       "       [ 0.2434 , -2.7332 , -5.7633 ],\n",
       "       [-0.99314,  1.41254, -0.35655]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1,2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]] #Transpose of weights matrix will help in multiplication\n",
    "\n",
    "#Both are now matrices\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "layer1_output = np.dot(np.array(inputs), np.array(weights).T) + biases #biases are added row-wise\n",
    " #output r1 = calc r1 + biases\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer2_outputs = np.dot(np.array(layer1_output), np.array(weights2).T) + biases2\n",
    "\n",
    "layer2_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can create a lot of code lines, instead we make an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n",
      "\n",
      "[[ 0.01110314  0.01753274  0.00116307 -0.04575625 -0.00087954  0.00599737\n",
      "   0.0287652   0.01130176  0.00780897 -0.00269652]\n",
      " [ 0.00359036  0.0072357  -0.01023086 -0.03054481 -0.0042225   0.00279179\n",
      "   0.01902682  0.01591173 -0.00329805 -0.00397739]\n",
      " [ 0.01076291  0.01796332 -0.00542691 -0.05408584 -0.00328605  0.00634035\n",
      "   0.03389322  0.01852949  0.00397137 -0.00453283]]\n"
     ]
    }
   ],
   "source": [
    "X = [[1,2, 3, 2.5],\n",
    "    [2.0, 5.0, -1.0, 2.0],\n",
    "    [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# We normalize weights between (-1, 1) so that the values are small while calculating\n",
    "# Start the bias as a non-zero nunmber if everything becomes zero for each neuron\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) # We put inputs before neuron numbers because it saves us from the hardwork of trnasposing\n",
    "        self.biases = np.zeros((1, n_neurons)) # we add a tuple to determine shape\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = Layer_Dense(4,5) #This column should be equal to\n",
    "layer2 = Layer_Dense(5,2) #This row\n",
    "layer3 = Layer_Dense(2,10)\n",
    "\n",
    "X_np = np.array(X)\n",
    "layer1.forward(X_np) # output has 3 rows because X has so\n",
    "print(layer1.output)\n",
    "print()\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)\n",
    "print()\n",
    "layer3.forward(layer2.output)\n",
    "print(layer3.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions / They come into play after the sum of inputs*weights.T + bias"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step function f(x) = 1,  if x>=0\n",
    "                     0   if x<0\n",
    "Every neuron has an activation function associated with it.\n",
    "Generally output layer has different activation function compared to hidden layers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sigmoid activation function f(x) = 1/(1+e^(-x))\n",
    "negative numbers become less than 0.5 when output and they come close to zero(0) as x -> -infinity\n",
    "and same goes for positive numbers, except they approach 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ReLU (Rectified Linear unit activation function)\n",
    "f(x) = 0,  if x<0\n",
    "       1   if x>=0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We mainly use ReLU because its fast, simple calculation compared to sigmoid.\n",
    "descent me dikkat deta hai sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "outputs = []\n",
    "for i in inputs:\n",
    "    '''\n",
    "    if i > 0:\n",
    "        outputs.append(i)\n",
    "    elif i <= 0:\n",
    "        outputs.append(0)\n",
    "    '''\n",
    "    #better way to code\n",
    "    outputs.append(max(0,i))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Object for ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nnfs\n",
    "#Why install this package? To repeat the results in the video\n",
    "\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "from nnfs.datasets import spiral_data\n",
    "X, y = spiral_data(100,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def create_data(points, classes):\n",
    "#     X = np.zeros((points*classes, 2))\n",
    "#     y = np.zeros(points*classes, dtype='uint8')\n",
    "#     for class_number in range(classes):\n",
    "#         ix = range(points*class_number, points*(class_number+1))\n",
    "#         r = np.linspace(0.0, 1, points)  # radius\n",
    "#         t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
    "#         X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "#         y[ix] = class_number\n",
    "#     return X, y\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(\"here\")\n",
    "# X, y = create_data(100, 3)\n",
    "\n",
    "# plt.scatter(X[:,0], X[:,1])\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(X[:,0], X[:,1], c=y, cmap='brg')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07766829 0.08039562 0.12873529 0.         0.        ]\n",
      " [0.13565907 0.         0.05062564 0.04318418 0.        ]\n",
      " [0.10540763 0.02938177 0.12049235 0.         0.        ]\n",
      " [0.12440884 0.         0.104376   0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer_Dense(2,5)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "\n",
    "print(activation1.output[96:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax activation function especially for the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "#Starting fresh for understanding this\n",
    "layer_outputs = [4.8, 1.21, 2.385] #Accurate\n",
    "\n",
    "# layer_outputs = [4.8, 4.79, 4.25] #Precise\n",
    "\n",
    "#Raw python code for exponentiation\n",
    "E = math.e\n",
    "\n",
    "exp_values = []\n",
    "\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "\n",
    "print(exp_values)\n",
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value/norm_base)\n",
    "\n",
    "print(norm_values)\n",
    "print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
      " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
      " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Next thing we're gonna do is to convert this into numpy\n",
    "layer_outputs = [[4.8, 1.21, 2.385],\n",
    "                 [8.9, -1.81, 0.2],\n",
    "                 [1.41, 1.051, 0.026]]\n",
    "\n",
    "exp_values = np.exp(layer_outputs) #every value in layer_outputs is exponentiated\n",
    "\n",
    "norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# print(exp_values)\n",
    "# print(np.sum(layer_outputs, axis = 1, keepdims = True)) #sum of a row, axis = 0 will be sum of a column\n",
    "print(norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of Exponentiation and Normalization is Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In exponentiation, overflow can easily occur with positive inputs as large as 1000. To avoid this, we subtract the max value in input array from all the elements so that the largest value in the input array is 0, exponentiation of which is 1. So all values in output are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333507, 0.33333328, 0.3333316 ],\n",
       "       [0.33327106, 0.3333301 , 0.33339888],\n",
       "       [0.33329675, 0.33330458, 0.33339873],\n",
       "       [0.33331198, 0.33331373, 0.33337426],\n",
       "       [0.33334184, 0.33333313, 0.33332503],\n",
       "       [0.33334365, 0.33333308, 0.3333232 ],\n",
       "       [0.3333453 , 0.33333308, 0.33332166],\n",
       "       [0.33334795, 0.333333  , 0.33331898],\n",
       "       [0.33335003, 0.33333293, 0.333317  ],\n",
       "       [0.33335102, 0.33333296, 0.33331606],\n",
       "       [0.33335316, 0.3333329 , 0.333314  ],\n",
       "       [0.3333543 , 0.33333287, 0.33331287],\n",
       "       [0.33335748, 0.3333328 , 0.3333097 ],\n",
       "       [0.33335924, 0.33333275, 0.333308  ],\n",
       "       [0.3333594 , 0.33333275, 0.33330783],\n",
       "       [0.33335876, 0.3333328 , 0.3333085 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33336312, 0.33333266, 0.33330417],\n",
       "       [0.33333904, 0.3333332 , 0.33332774],\n",
       "       [0.3333392 , 0.33333316, 0.33332756],\n",
       "       [0.3333669 , 0.33333257, 0.3333005 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333648 , 0.33333263, 0.33330256],\n",
       "       [0.33337796, 0.33333236, 0.33328974],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33336833, 0.33333254, 0.33329907],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333677, 0.33333325, 0.33332998],\n",
       "       [0.3322998 , 0.33416602, 0.33353418],\n",
       "       [0.33260375, 0.3340488 , 0.3333475 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3330484 , 0.33361256, 0.333339  ],\n",
       "       [0.33274266, 0.33391246, 0.33334488],\n",
       "       [0.33263034, 0.33402267, 0.333347  ],\n",
       "       [0.33188736, 0.3344303 , 0.3336823 ],\n",
       "       [0.33122313, 0.3345878 , 0.33418912],\n",
       "       [0.33119133, 0.33461764, 0.334191  ],\n",
       "       [0.3321644 , 0.33440644, 0.3334292 ],\n",
       "       [0.3325906 , 0.33406168, 0.33334777],\n",
       "       [0.33035484, 0.33447433, 0.3351708 ],\n",
       "       [0.33121377, 0.33470917, 0.3340771 ],\n",
       "       [0.33090907, 0.33478326, 0.33430767],\n",
       "       [0.33054993, 0.33484063, 0.33460942],\n",
       "       [0.33179268, 0.3346263 , 0.33358097],\n",
       "       [0.33012775, 0.33487314, 0.3349991 ],\n",
       "       [0.32997516, 0.33486447, 0.33516037],\n",
       "       [0.33018553, 0.33427462, 0.33553988],\n",
       "       [0.33036965, 0.33409333, 0.33553705],\n",
       "       [0.32990777, 0.3344553 , 0.33563694],\n",
       "       [0.3310561 , 0.33359915, 0.3353448 ],\n",
       "       [0.33047605, 0.33393776, 0.3355862 ],\n",
       "       [0.33023724, 0.3340773 , 0.33568546],\n",
       "       [0.3306756 , 0.33376265, 0.33556172],\n",
       "       [0.3304154 , 0.3339049 , 0.33567968],\n",
       "       [0.33036038, 0.33391696, 0.33572266],\n",
       "       [0.33081493, 0.3336172 , 0.3355679 ],\n",
       "       [0.3298278 , 0.3342361 , 0.33593613],\n",
       "       [0.33257264, 0.33273795, 0.3346894 ],\n",
       "       [0.3304804 , 0.33375114, 0.3357685 ],\n",
       "       [0.3333117 , 0.33324483, 0.33344343],\n",
       "       [0.33259144, 0.33274668, 0.33466187],\n",
       "       [0.3329826 , 0.33301365, 0.33400378],\n",
       "       [0.3334427 , 0.33333087, 0.3332264 ],\n",
       "       [0.33345208, 0.33333066, 0.3332172 ],\n",
       "       [0.3315183 , 0.33308578, 0.33539587],\n",
       "       [0.333445  , 0.33333084, 0.3332242 ],\n",
       "       [0.33346102, 0.33333048, 0.3332085 ],\n",
       "       [0.33248013, 0.3326619 , 0.33485797],\n",
       "       [0.33346546, 0.3333304 , 0.33320418],\n",
       "       [0.33219698, 0.33268368, 0.3351193 ],\n",
       "       [0.33345896, 0.33333054, 0.3332105 ],\n",
       "       [0.33346808, 0.3333303 , 0.3332016 ],\n",
       "       [0.3334349 , 0.3333158 , 0.33324927],\n",
       "       [0.33347502, 0.33333018, 0.33319482],\n",
       "       [0.3334738 , 0.33333018, 0.333196  ],\n",
       "       [0.3333526 , 0.3333329 , 0.3333145 ],\n",
       "       [0.33343652, 0.33333102, 0.3332324 ],\n",
       "       [0.3334012 , 0.3333318 , 0.333267  ],\n",
       "       [0.33343822, 0.333331  , 0.3332308 ],\n",
       "       [0.33343548, 0.33333102, 0.33323348],\n",
       "       [0.3334811 , 0.33333   , 0.3331889 ],\n",
       "       [0.33340815, 0.33333164, 0.3332602 ],\n",
       "       [0.33342704, 0.33333126, 0.33324176],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33318725, 0.33347648, 0.33333623],\n",
       "       [0.33207634, 0.33456662, 0.3333571 ],\n",
       "       [0.33293456, 0.33372423, 0.3333412 ],\n",
       "       [0.33334067, 0.33333316, 0.33332616],\n",
       "       [0.33336735, 0.33333257, 0.33330008],\n",
       "       [0.33179033, 0.3348477 , 0.33336204],\n",
       "       [0.33222285, 0.3344227 , 0.33335444],\n",
       "       [0.3333725 , 0.33333248, 0.33329508],\n",
       "       [0.33132645, 0.33530375, 0.33336976],\n",
       "       [0.32955727, 0.3361878 , 0.33425495],\n",
       "       [0.3310595 , 0.3355665 , 0.33337405],\n",
       "       [0.33112395, 0.33550304, 0.333373  ],\n",
       "       [0.3320393 , 0.33460298, 0.33335772],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3332739 , 0.33336678, 0.33335936],\n",
       "       [0.3332614 , 0.33339092, 0.3333477 ],\n",
       "       [0.33320075, 0.33342606, 0.3333732 ],\n",
       "       [0.33305174, 0.33343732, 0.33351094],\n",
       "       [0.33298624, 0.33349538, 0.3335184 ],\n",
       "       [0.33292326, 0.3335304 , 0.33354634],\n",
       "       [0.33285695, 0.33356383, 0.33357918],\n",
       "       [0.3328249 , 0.33348233, 0.3336928 ],\n",
       "       [0.33283174, 0.3334475 , 0.33372077],\n",
       "       [0.33267006, 0.33354467, 0.33378527],\n",
       "       [0.33258435, 0.33358595, 0.33382973],\n",
       "       [0.33281776, 0.33370072, 0.33348152],\n",
       "       [0.33244085, 0.3336403 , 0.33391884],\n",
       "       [0.3323653 , 0.3336724 , 0.33396226],\n",
       "       [0.33237737, 0.33361492, 0.3340077 ],\n",
       "       [0.33271477, 0.33337018, 0.33391502],\n",
       "       [0.3324389 , 0.3338874 , 0.33367372],\n",
       "       [0.33217013, 0.33368522, 0.33414462],\n",
       "       [0.33255982, 0.33339834, 0.3340419 ],\n",
       "       [0.33223534, 0.3335754 , 0.33418924],\n",
       "       [0.33190906, 0.33380955, 0.33428138],\n",
       "       [0.3331473 , 0.3331784 , 0.33367434],\n",
       "       [0.3330256 , 0.33310142, 0.33387297],\n",
       "       [0.3321973 , 0.33350685, 0.33429584],\n",
       "       [0.33313838, 0.33316857, 0.33369306],\n",
       "       [0.333373  , 0.3333304 , 0.33329657],\n",
       "       [0.3331555 , 0.3331779 , 0.3336666 ],\n",
       "       [0.33338308, 0.33333224, 0.33328474],\n",
       "       [0.3333246 , 0.33329293, 0.3333825 ],\n",
       "       [0.33321622, 0.33321625, 0.3335675 ],\n",
       "       [0.33299044, 0.33305985, 0.33394974],\n",
       "       [0.33338937, 0.3333321 , 0.33327854],\n",
       "       [0.33339462, 0.33333197, 0.3332734 ],\n",
       "       [0.33339357, 0.33333197, 0.3332744 ],\n",
       "       [0.3333986 , 0.33333185, 0.33326948],\n",
       "       [0.3333973 , 0.3333319 , 0.33327082],\n",
       "       [0.33339956, 0.33333185, 0.33326855],\n",
       "       [0.33340403, 0.33333173, 0.3332642 ],\n",
       "       [0.3334022 , 0.3333318 , 0.33326602],\n",
       "       [0.3334081 , 0.33333167, 0.33326024],\n",
       "       [0.3333745 , 0.33333245, 0.33329308],\n",
       "       [0.33338052, 0.33333227, 0.33328718],\n",
       "       [0.3333811 , 0.33333227, 0.3332867 ],\n",
       "       [0.33337313, 0.33333245, 0.33329445],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33338916, 0.33333206, 0.33327875],\n",
       "       [0.3334208 , 0.33333138, 0.3332478 ],\n",
       "       [0.33334854, 0.33333302, 0.33331844],\n",
       "       [0.33335102, 0.33333296, 0.33331606],\n",
       "       [0.33336335, 0.33333266, 0.333304  ],\n",
       "       [0.33337536, 0.33333242, 0.33329222],\n",
       "       [0.33334166, 0.33333313, 0.33332524],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33012134, 0.33516535, 0.33471328],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.333376  , 0.33333236, 0.33329165],\n",
       "       [0.33201355, 0.33462828, 0.33335817],\n",
       "       [0.33334073, 0.33333316, 0.33332607],\n",
       "       [0.3325001 , 0.33415052, 0.3333494 ],\n",
       "       [0.33184212, 0.33479673, 0.33336115],\n",
       "       [0.33213603, 0.33450794, 0.33335602],\n",
       "       [0.32877305, 0.335243  , 0.33598396],\n",
       "       [0.3289209 , 0.33546597, 0.33561316],\n",
       "       [0.32863268, 0.33531013, 0.33605722],\n",
       "       [0.33094338, 0.3352587 , 0.3337979 ],\n",
       "       [0.32849327, 0.33525005, 0.3362567 ],\n",
       "       [0.32847428, 0.33549893, 0.3360268 ],\n",
       "       [0.3283596 , 0.33527598, 0.33636442],\n",
       "       [0.32851493, 0.33494997, 0.33653507],\n",
       "       [0.328972  , 0.3357404 , 0.33528757],\n",
       "       [0.3282472 , 0.3356678 , 0.33608502],\n",
       "       [0.32878333, 0.33458367, 0.336633  ],\n",
       "       [0.33103   , 0.3353806 , 0.3335894 ],\n",
       "       [0.33069763, 0.33334368, 0.33595872],\n",
       "       [0.32906085, 0.33429253, 0.33664665],\n",
       "       [0.32840365, 0.33476114, 0.33683518],\n",
       "       [0.32907897, 0.3342293 , 0.3366917 ],\n",
       "       [0.3277365 , 0.33537507, 0.3368885 ],\n",
       "       [0.32867292, 0.3344593 , 0.3368678 ],\n",
       "       [0.3324222 , 0.3326067 , 0.33497113],\n",
       "       [0.3328289 , 0.33288443, 0.33428663],\n",
       "       [0.33349043, 0.3333298 , 0.33317974],\n",
       "       [0.33103204, 0.33301756, 0.3359504 ],\n",
       "       [0.32994345, 0.33356106, 0.33649552],\n",
       "       [0.33087173, 0.33306226, 0.33606598],\n",
       "       [0.32998538, 0.33350095, 0.3365137 ],\n",
       "       [0.3326052 , 0.33272308, 0.33467165],\n",
       "       [0.33337978, 0.33325937, 0.33336085],\n",
       "       [0.33311996, 0.3330767 , 0.3338033 ],\n",
       "       [0.3335054 , 0.33332947, 0.33316517],\n",
       "       [0.33336782, 0.33324745, 0.3333847 ],\n",
       "       [0.33347932, 0.33332443, 0.3331963 ],\n",
       "       [0.33344066, 0.33329615, 0.3332632 ],\n",
       "       [0.33237863, 0.33255896, 0.33506235],\n",
       "       [0.33269665, 0.33277613, 0.3345272 ],\n",
       "       [0.3334547 , 0.3333024 , 0.33324286],\n",
       "       [0.33345965, 0.3333305 , 0.3332098 ],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333442, 0.3333333 , 0.33333227],\n",
       "       [0.33333692, 0.33333325, 0.33332983],\n",
       "       [0.33333778, 0.3333329 , 0.3333293 ],\n",
       "       [0.3333406 , 0.3333332 , 0.33332622],\n",
       "       [0.33334008, 0.33333316, 0.33332673],\n",
       "       [0.33333915, 0.3333332 , 0.33332762],\n",
       "       [0.33333555, 0.33333328, 0.33333117],\n",
       "       [0.33333984, 0.33333316, 0.33332697],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33334234, 0.3333331 , 0.33332452],\n",
       "       [0.33334732, 0.33333302, 0.33331966],\n",
       "       [0.33335257, 0.3333329 , 0.3333145 ],\n",
       "       [0.33334568, 0.33333308, 0.33332124],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33334243, 0.33333313, 0.33332443],\n",
       "       [0.3331831 , 0.33348057, 0.33333635],\n",
       "       [0.33291322, 0.33374515, 0.3333416 ],\n",
       "       [0.33335674, 0.33333284, 0.33331048],\n",
       "       [0.33292237, 0.3337362 , 0.33334145],\n",
       "       [0.33335128, 0.33333293, 0.33331576],\n",
       "       [0.33290607, 0.33375221, 0.33334178],\n",
       "       [0.33226365, 0.33403435, 0.33370203],\n",
       "       [0.33283144, 0.33382538, 0.33334318],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33292532, 0.3337333 , 0.3333414 ],\n",
       "       [0.33259448, 0.33402416, 0.33338135],\n",
       "       [0.33232534, 0.33412114, 0.33355346],\n",
       "       [0.33213788, 0.33418888, 0.33367324],\n",
       "       [0.33186454, 0.33426863, 0.3338669 ],\n",
       "       [0.33147362, 0.33433875, 0.3341876 ],\n",
       "       [0.33238873, 0.3341767 , 0.33343458],\n",
       "       [0.33137974, 0.334405  , 0.33421525],\n",
       "       [0.33119488, 0.33398375, 0.33482137],\n",
       "       [0.33111122, 0.33446786, 0.33442086],\n",
       "       [0.3309799 , 0.3344905 , 0.3345296 ],\n",
       "       [0.3310984 , 0.3339594 , 0.33494216],\n",
       "       [0.33236647, 0.33428168, 0.33335185],\n",
       "       [0.3313863 , 0.33370045, 0.33491322],\n",
       "       [0.33055001, 0.33449587, 0.33495417],\n",
       "       [0.33077917, 0.33408824, 0.3351326 ],\n",
       "       [0.33080927, 0.33402923, 0.33516154],\n",
       "       [0.33186793, 0.33334494, 0.33478716],\n",
       "       [0.33071804, 0.33403796, 0.335244  ],\n",
       "       [0.33050087, 0.33418372, 0.3353154 ],\n",
       "       [0.33292025, 0.33299404, 0.3340857 ],\n",
       "       [0.3301108 , 0.33479792, 0.33509126],\n",
       "       [0.33182737, 0.3332769 , 0.3348957 ],\n",
       "       [0.33035344, 0.33416784, 0.33547866],\n",
       "       [0.33118924, 0.3335829 , 0.33522782],\n",
       "       [0.3334267 , 0.33333126, 0.3332421 ],\n",
       "       [0.332783  , 0.33289272, 0.33432427],\n",
       "       [0.33306345, 0.33308405, 0.3338525 ],\n",
       "       [0.3307093 , 0.3337864 , 0.33550432],\n",
       "       [0.33269948, 0.33283198, 0.33446857],\n",
       "       [0.3334255 , 0.3333313 , 0.33324322],\n",
       "       [0.333434  , 0.33333108, 0.3332349 ],\n",
       "       [0.3334398 , 0.33333096, 0.33322924],\n",
       "       [0.3334384 , 0.333331  , 0.3332306 ],\n",
       "       [0.33344302, 0.33333087, 0.3332261 ],\n",
       "       [0.33344385, 0.33333087, 0.33322525],\n",
       "       [0.33285394, 0.3329287 , 0.3342174 ],\n",
       "       [0.3334477 , 0.33333078, 0.33322155],\n",
       "       [0.33342826, 0.3333312 , 0.33324048],\n",
       "       [0.33342895, 0.3333312 , 0.33323985],\n",
       "       [0.33345053, 0.33333072, 0.3332188 ],\n",
       "       [0.33345538, 0.33333063, 0.333214  ],\n",
       "       [0.33345807, 0.33333054, 0.33321136],\n",
       "       [0.33343148, 0.33333114, 0.33323738],\n",
       "       [0.33335245, 0.33333293, 0.33331466],\n",
       "       [0.33346424, 0.33333042, 0.33320537],\n",
       "       [0.33343247, 0.3333311 , 0.3332364 ],\n",
       "       [0.3334453 , 0.33333084, 0.33322388],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.3333981 , 0.33333188, 0.33327   ],\n",
       "       [0.33338067, 0.33333227, 0.33328706],\n",
       "       [0.33334985, 0.33333296, 0.33331722],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333364, 0.33333334, 0.33333305],\n",
       "       [0.3334725 , 0.3333302 , 0.3331973 ],\n",
       "       [0.3326408 , 0.33401242, 0.33334678],\n",
       "       [0.3324437 , 0.33420584, 0.33335042],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33142224, 0.33520955, 0.3333682 ],\n",
       "       [0.33093777, 0.3355878 , 0.33347446],\n",
       "       [0.33218452, 0.33446035, 0.33335516],\n",
       "       [0.33128434, 0.3353452 , 0.33337045],\n",
       "       [0.33227354, 0.3343729 , 0.33335352],\n",
       "       [0.3301178 , 0.33590302, 0.3339792 ],\n",
       "       [0.332713  , 0.33394155, 0.3333454 ],\n",
       "       [0.32777262, 0.33637533, 0.33585203],\n",
       "       [0.3304709 , 0.3358565 , 0.33367264],\n",
       "       [0.3273061 , 0.33643162, 0.3362623 ],\n",
       "       [0.33058333, 0.3358582 , 0.33355844],\n",
       "       [0.32790273, 0.3364844 , 0.33561283],\n",
       "       [0.32680145, 0.3364691 , 0.3367294 ],\n",
       "       [0.3294387 , 0.3362379 , 0.33432338],\n",
       "       [0.32635832, 0.3361609 , 0.33748078],\n",
       "       [0.32634646, 0.3364021 , 0.3372514 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating a class for softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #This subtraction is performed to prevent overflow\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "(activation2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Along with softmax activation the loss function which goes with it is called\n",
    "Categorical Cross-Entropy\n",
    "\n",
    "L_i = -/sigma_of_j y_i,jlog(y/hat_i,j)\n",
    "y = target value\n",
    "y/hat = predicted value\n",
    "\n",
    "this formula reduces to L_i = -log(y/hat_i,j) due to one hot encoding\n",
    "\n",
    "this formula is famous and is convenient for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss :  1.09855\n"
     ]
    }
   ],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # [1,0]  #We wanna handle both of these\n",
    "        # [[0,1],[1,0]]\n",
    "\n",
    "        if len(y_true.shape) == 1: #Scalar class values handling\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2: #one hot encoded vectors handling\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "print(\"Loss : \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
