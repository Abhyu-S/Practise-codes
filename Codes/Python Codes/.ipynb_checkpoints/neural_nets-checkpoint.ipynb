{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "049f863f-6aab-494a-a545-d015b1bb1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14a5753-5798-47f3-a9aa-1cedb4ce83cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PatientId', 'AppointmentID', 'Gender', 'ScheduledDay',\n",
       "       'AppointmentDay', 'Age', 'Neighbourhood', 'Scholarship', 'Hipertension',\n",
       "       'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received', 'No-show'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r\"C:/Users/VICTUS/vs items/Codes/Python Codes/archive/KaggleV2-May-2016.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea45603d-7470-4462-82ff-c66d2881f022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110527,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['No-show'] == 'Yes', 'No-show'] = 1\n",
    "df.loc[df['No-show'] == 'No', 'No-show'] = 0\n",
    "df.loc[df['Gender'] == 'M', 'Gender'] = 1\n",
    "df.loc[df['Gender'] == 'F', 'Gender'] = 2\n",
    "\n",
    "features = ['Gender', 'Age', 'Scholarship', 'Hipertension', 'Diabetes', 'Alcoholism', 'Handcap', 'SMS_received']\n",
    "\n",
    "y = df['No-show']\n",
    "\n",
    "X = df[features].astype(np.float64).values  # Convert to numpy array\n",
    "X = np.nan_to_num(X)  # Replace NaNs with 0 if any remain\n",
    "\n",
    "y = y.astype(int).values\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d764e18-2f0b-440d-9bc5-4489bbb3e9f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110527, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0559045-6e2c-4086-9632-4c7b3bfe4900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5493, Accuracy: 0.7776\n",
      "Epoch 10, Loss: 0.5341, Accuracy: 0.7860\n",
      "Epoch 20, Loss: 0.5332, Accuracy: 0.7959\n",
      "Epoch 30, Loss: 0.5323, Accuracy: 0.7979\n",
      "Epoch 40, Loss: 0.5315, Accuracy: 0.7980\n",
      "Epoch 50, Loss: 0.5307, Accuracy: 0.7980\n",
      "Epoch 60, Loss: 0.5300, Accuracy: 0.7981\n",
      "Epoch 70, Loss: 0.5292, Accuracy: 0.7981\n",
      "Epoch 80, Loss: 0.5285, Accuracy: 0.7981\n",
      "Epoch 90, Loss: 0.5278, Accuracy: 0.7981\n",
      "Epoch 100, Loss: 0.5271, Accuracy: 0.7981\n",
      "Epoch 110, Loss: 0.5264, Accuracy: 0.7981\n",
      "Epoch 120, Loss: 0.5257, Accuracy: 0.7981\n",
      "Epoch 130, Loss: 0.5251, Accuracy: 0.7981\n",
      "Epoch 140, Loss: 0.5245, Accuracy: 0.7981\n",
      "Epoch 150, Loss: 0.5239, Accuracy: 0.7981\n",
      "Epoch 160, Loss: 0.5233, Accuracy: 0.7981\n",
      "Epoch 170, Loss: 0.5227, Accuracy: 0.7981\n",
      "Epoch 180, Loss: 0.5222, Accuracy: 0.7981\n",
      "Epoch 190, Loss: 0.5216, Accuracy: 0.7981\n",
      "Epoch 200, Loss: 0.5211, Accuracy: 0.7981\n",
      "Epoch 210, Loss: 0.5206, Accuracy: 0.7981\n",
      "Epoch 220, Loss: 0.5201, Accuracy: 0.7981\n",
      "Epoch 230, Loss: 0.5196, Accuracy: 0.7981\n",
      "Epoch 240, Loss: 0.5191, Accuracy: 0.7981\n",
      "Epoch 250, Loss: 0.5186, Accuracy: 0.7981\n",
      "Epoch 260, Loss: 0.5182, Accuracy: 0.7981\n",
      "Epoch 270, Loss: 0.5177, Accuracy: 0.7981\n",
      "Epoch 280, Loss: 0.5173, Accuracy: 0.7981\n",
      "Epoch 290, Loss: 0.5169, Accuracy: 0.7981\n",
      "Epoch 300, Loss: 0.5165, Accuracy: 0.7981\n",
      "Epoch 310, Loss: 0.5161, Accuracy: 0.7981\n",
      "Epoch 320, Loss: 0.5157, Accuracy: 0.7981\n",
      "Epoch 330, Loss: 0.5153, Accuracy: 0.7981\n",
      "Epoch 340, Loss: 0.5149, Accuracy: 0.7981\n",
      "Epoch 350, Loss: 0.5146, Accuracy: 0.7981\n",
      "Epoch 360, Loss: 0.5142, Accuracy: 0.7981\n",
      "Epoch 370, Loss: 0.5139, Accuracy: 0.7981\n",
      "Epoch 380, Loss: 0.5135, Accuracy: 0.7981\n",
      "Epoch 390, Loss: 0.5132, Accuracy: 0.7981\n",
      "Epoch 400, Loss: 0.5129, Accuracy: 0.7981\n",
      "Epoch 410, Loss: 0.5126, Accuracy: 0.7981\n",
      "Epoch 420, Loss: 0.5123, Accuracy: 0.7981\n"
     ]
    }
   ],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.10*np.random.randn(n_inputs, n_neurons) # We put inputs before neuron numbers because it saves us from the hardwork of trnasposing\n",
    "        self.biases = np.zeros((1, n_neurons)) # we add a tuple to determine shape\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True)) #This subtraction is performed to prevent overflow\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_loss = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_loss)\n",
    "        return sample_loss\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1: #Scalar class values handling\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2: #one hot encoded vectors handling\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "# 1/N * (w*x - y)**2\n",
    "\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "layer1 = Layer_Dense(8, 16)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer2 = Layer_Dense(16, 2)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Training step\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- FORWARD PASS ----\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "\n",
    "    # ---- LOSS ----\n",
    "    loss = loss_function.forward(activation2.output, y)\n",
    "    avg_loss = np.mean(loss)\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # ---- BACKWARD PASS (manual gradient descent) ----\n",
    "\n",
    "    # One-hot case\n",
    "    if len(y.shape) == 1:\n",
    "        y_true = np.eye(2)[y]  # Convert scalar class to one-hot\n",
    "    else:\n",
    "        y_true = y\n",
    "\n",
    "    # Gradient of loss w.r.t. output (Softmax + CrossEntropy derivative)\n",
    "    dvalues = activation2.output - y_true\n",
    "    dvalues /= X.shape[0]  # Normalize gradient by batch size\n",
    "\n",
    "    # Gradients for layer2\n",
    "    dweights2 = np.dot(activation1.output.T, dvalues)\n",
    "    dbiases2 = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "    # Gradient flowing into layer1\n",
    "    dactivation1 = np.dot(dvalues, layer2.weights.T)\n",
    "    dactivation1[activation1.output <= 0] = 0  # ReLU derivative\n",
    "\n",
    "    dweights1 = np.dot(X.T, dactivation1)\n",
    "    dbiases1 = np.sum(dactivation1, axis=0, keepdims=True)\n",
    "\n",
    "    # ---- WEIGHT UPDATE ----\n",
    "    layer2.weights -= learning_rate * dweights2\n",
    "    layer2.biases -= learning_rate * dbiases2\n",
    "\n",
    "    layer1.weights -= learning_rate * dweights1\n",
    "    layer1.biases -= learning_rate * dbiases1\n",
    "\n",
    "print(\"Training over!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09aa1e66-9ed6-4717-a181-9b58b771407e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.798067440534892"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_function.calculate(activation2.output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529929a-5fc9-42ff-94e5-ea0b652a628f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
